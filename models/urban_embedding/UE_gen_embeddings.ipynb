{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import sys\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "sys.path.extend(['/content/drive/MyDrive/KAIST/thesis/models'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from urban_embedding.UE import VAE_Encoder, Transformer\n",
    "\n",
    "dir_path = '/content/drive/MyDrive/KAIST/thesis'\n",
    "data_path = dir_path + '/data'\n",
    "reconstruction_loss = 'MAE' # 'MSE' # reconstruction loss used to train the model\n",
    "load_path = dir_path + f'/models/urban_embedding/UE_{reconstruction_loss}.model'\n",
    "offs_months = 7\n",
    "Q = 1 # output sequence length\n",
    "D = 64 # latent space dimension\n",
    "L = 3 # number of STAtt blocks in the encoder/decoder\n",
    "K = 8 # number of attention heads\n",
    "d = 8 # dimension of each attention head outputs\n",
    "\n",
    "# urban features\n",
    "UF: np.ndarray = np.load(data_path + '/urban_features.npy') # (M, N, F)\n",
    "M, N, F = UF.shape\n",
    "P_max = M - offs_months - Q + 1\n",
    "UF = np.transpose(UF, (1, 0, 2)) # (N, M, F)\n",
    "\n",
    "encoder = VAE_Encoder(D, F, K*d)\n",
    "encoder.load_weights(load_path.replace('.model', '_encoder.model'))\n",
    "transformer = Transformer(1, P_max, D, L, K, d)\n",
    "transformer.load_weights(load_path.replace('.model', '_transformer.model'))\n",
    "embeddings = []\n",
    "for n in range(N):\n",
    "  batch = []\n",
    "  for m in range(1, P_max+1):\n",
    "    features = UF[n, :m, :] # (m, F)\n",
    "    features = tf.pad(features, [[P_max - m, 0], [0,0]], 'CONSTANT') # (P_max, F)\n",
    "    batch.append(features) \n",
    "  batch = np.stack(batch) # (P_max, P_max, F)\n",
    "  assert batch.shape == (P_max, P_max, F)\n",
    "  _, _, P_embeddings = encoder.encode(batch) # (P_max, P_max, D)\n",
    "  assert P_embeddings.shape == (P_max, P_max, D)\n",
    "  Q_embeddings = transformer(P_embeddings) # (P_max, 1, D)\n",
    "  assert Q_embeddings.shape == (P_max, 1, D)\n",
    "  P_embeddings = P_embeddings[:, -1, :] # (P_max, D)\n",
    "  assert P_embeddings.shape == (P_max, D)\n",
    "  P_embeddings = np.expand_dims(P_embeddings, 1) # (P_max, 1, D)\n",
    "  assert P_embeddings.shape == (P_max, 1, D)\n",
    "  PQ_embeddings = np.concatenate([P_embeddings, Q_embeddings], axis=1) # (P_max, 2, D)\n",
    "  assert PQ_embeddings.shape == (P_max, 2, D)\n",
    "  embeddings.append(PQ_embeddings)\n",
    "embeddings = np.stack(embeddings) # (N, P_max, 2, D)\n",
    "assert embeddings.shape == (N, P_max, 2, D)\n",
    "embeddings = np.transpose(embeddings, (1, 2, 0, 3)) # (P_max, 2, N, D)\n",
    "assert embeddings.shape == (P_max, 2, N, D)\n",
    "assert not np.isnan(np.sum(embeddings))\n",
    "np.save(data_path + f'/urban_embeddings_{reconstruction_loss}.npy', embeddings)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
